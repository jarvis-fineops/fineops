# ğŸš€ Deploying to Kubernetes Without Taking Down Prod

Kubernetes gives you rolling updates, but donâ€™t assume it guarantees uptime. Itâ€™s more likeâ€”hereâ€™s the sharp knife, use it wisely.

Weâ€™ve seen folks deploy what looks like a safe change, only to get alert spam minutes later. Because K8s happily routed traffic to pods that werenâ€™t ready, or killed too many at once.

This isnâ€™t about being fancy. Itâ€™s about knowing which switches matter when prod is on the line.

---

## âœ… Step 1: Add Readiness & Liveness Probes (Proper Ones)

**Donâ€™t skip these.** Theyâ€™re the only things standing between a graceful deploy and a mid-rollout meltdown.

- **Readiness probe** â†’ tells Kubernetes when the pod can handle traffic
- **Liveness probe** â†’ tells Kubernetes when the pod needs to be restarted

Example:
```yaml
readinessProbe:
  httpGet:
    path: /ready
    port: 8080
  initialDelaySeconds: 3
  periodSeconds: 5
```

If youâ€™re not using these, K8s is just guessing when to let your app start taking traffic.

---

## âš™ï¸ Step 2: Tune Your Rollout Strategy

Kubernetes will try to keep your app aliveâ€”but youâ€™ve gotta tell it how.

```yaml
strategy:
  type: RollingUpdate
  rollingUpdate:
    maxSurge: 1
    maxUnavailable: 0
```

This setup means:
- Itâ€™ll add 1 extra pod during the rollout
- It wonâ€™t take any old pods offline until a new one is ready

For apps with 2â€“3 replicas, this is critical. You need all your pods alive until the new ones are 100% good.

---

## ğŸ“Š Step 3: Watch Your Metrics (Seriously)

Donâ€™t just rely on K8s saying â€œRollout Complete.â€ Look at your actual app:

- Are 5xx errors spiking?
- Is response time climbing?
- Are any pods restarting unexpectedly?

Use whatever tools youâ€™ve gotâ€”Prometheus, Datadog, Grafanaâ€”but keep an eye during deploys.

---

## ğŸ¢ Step 4: Go Slower If You Need To

If this update feels risky, throttle it. You donâ€™t need to blast 100% of traffic at your new version immediately.

Options:
- Lower `maxSurge` to 0
- Use Argo Rollouts or Flagger for canary-style deploys
- Roll out in chunks by node pool or zone

Itâ€™s easier to catch bugs at 10% traffic than at full blast.

---

## ğŸš« Things That Break Rollouts

- No readiness probe â€” users hit a cold booting pod
- Over-aggressive surge/unavailable settings
- Monitoring turned off during deploys
- Fixes made in staging that never made it to prod YAML

---


## TL;DR

Kubernetes gives you the knobsâ€”you just have to turn the right ones.

- Set up probes that actually reflect your appâ€™s health
- Use rollout settings that keep your pods alive
- Watch your metrics while rolling out
- Donâ€™t rush it if itâ€™s risky

Shipping to prod doesnâ€™t have to be scary. But it *can* be if you ignore the basics.
